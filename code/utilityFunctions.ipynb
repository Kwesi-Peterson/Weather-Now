{"cells":[{"cell_type":"code","source":["# Utility Functions\nimport json\nimport requests\nimport logging\nfrom datetime import datetime, timedelta\n\ndef getWeatherData(latStr, lonStr, unitsStr, appIdStr, excludeStr):\n    \"\"\"\n    Get weather data via the OpenWeatherMap API.\n    \n    Returns: Spark DataFrame\n    \n    \"\"\"\n    assert type(latStr) == str\n    assert type(lonStr) == str\n    assert type(unitsStr) == str\n    assert type(appIdStr) == str\n    assert type(excludeStr) == str\n    url = \"https://api.openweathermap.org/data/2.5/onecall?lat=\" + latStr + \"&lon=\" + lonStr + \"&units=\" + unitsStr + \"&exclude=\" + excludeStr + \"&appid=\" + appIdStr\n    \n    myResponse = requests.get(url)\n    \n    if myResponse.ok:\n        jDataRaw = json.loads(myResponse.content)\n        jDataStr = json.dumps(jDataRaw)\n        jDataList = []\n        jDataList.append(jDataStr)\n        jRDD = sc.parallelize(jDataList)\n        jDataFrame = spark.read.json(jRDD)\n        return jDataFrame\n    else:\n        print(\"URL request failed, error code: \" + str(myResponse.status_code))\n        \n\ndef mountStorage(accountName, containerName, folderName, mountPoint, scopeName, applicationIDKeyName, clientIDKeyName, tenantIDKeyName):\n    \"\"\"\n    Mount the ADLS gen 2 storage file\n    \n    Returns: None\n    \"\"\"\n    # Application (Client) ID\n    applicationId = dbutils.secrets.get(scope=scopeName,key=applicationIDKeyName)\n\n    # Application (Client) Secret Key\n    authenticationKey = dbutils.secrets.get(scope=scopeName,key=clientIDKeyName)\n\n    # Directory (Tenant) ID\n    tenantId = dbutils.secrets.get(scope=scopeName,key=tenantIDKeyName)\n\n    endpoint = \"https://login.microsoftonline.com/\" + tenantId + \"/oauth2/token\"\n    source = \"abfss://\" + containerName + \"@\" + accountName + \".dfs.core.windows.net/\" + folderName\n\n    # Connecting using Service Principal secrets and OAuth\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n               \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n               \"fs.azure.account.oauth2.client.id\": applicationId,\n               \"fs.azure.account.oauth2.client.secret\": authenticationKey,\n               \"fs.azure.account.oauth2.client.endpoint\": endpoint}\n\n    # Mounting ADLS Storage to DBFS\n    # Mount only if the directory is not already mounted\n    if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n        dbutils.fs.mount(\n            source=source,\n            mount_point=mountPoint,\n            extra_configs=configs)\n    else:\n        print(\"We have already mounted the mount point \" + mountPoint)\n        \n\ndef unmountStorage(mountPoint):\n    \"\"\"\n    Unmount the ADLS gen 2 storage file\n    \n    Returns: None\n    \"\"\"\n    # Unmount only if directory is mounted\n    if any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n        dbutils.fs.unmount(mountPoint)\n    else:\n        print(\"The mount point \" + mountPoint + \" isn't currently mounted\")\n\n        \ndef writeDfToDeltaTable(dataframe, databaseName, tableName, writeMode, partitionColumn):\n    \"\"\"\n    Write Spark Data Frame to Delta table\n    \n    Returns: None\n    \"\"\"\n    location = databaseName + \".\" + tableName\n    dataframe.write.format(\"delta\").mode(writeMode).partitionBy(partitionColumn).saveAsTable(location)\n    \n    \ndef writeDfToStorage(dataFrame, storagePath):\n    \"\"\"\n    Write Spark Data Frame to Storage Account\n    \n    Returns: None\n    \"\"\"\n    dataFrame.write.format('csv').mode('overwrite').option('header', True).save(storagePath)\n    \n    \ndef getCurrentTimeUTC():\n    \"\"\"\n    Get the current time in UTC in the format yyyymmddhh\n    \n    Returns: Current time in UTC (in format yyyymmddhh) in string format\n    \n    \"\"\"\n    return datetime.utcnow().strftime('%Y%m%d%H')\n\n\ndef getCurrentYearUTC():\n    \"\"\"\n    Get the current year\n    \n    Returns: Current year (in format yyyy) in string format\n    \n    \"\"\"\n    return datetime.utcnow().strftime('%Y')\n\n\ndef getCurrentMonthUTC():\n    \"\"\"\n    Get the current month\n    \n    Returns: Current month (in format mm) in string format\n    \n    \"\"\"\n    return datetime.utcnow().strftime('%m')\n\n\ndef getCurrentDayUTC():\n    \"\"\"\n    Get the current day\n    \n    Returns: Current month (in format dd) in string format\n    \n    \"\"\"\n    return datetime.utcnow().strftime('%d')\n  \n    \ndef getLastNDays(numDaysBack):\n    \"\"\"\n    Get a list of the last N days\n    \n    Returns: A list of the last N dates (in format yyyy-mm-dd)\n    \n    \"\"\"\n    dayList = []\n    for counter in range(0, numDaysBack):\n        date = (datetime.utcnow() - timedelta(days=counter)).strftime('%Y-%m-%d')\n        dayList.append(date)\n    return dayList\n\n\ndef getLastNHrs(numHrsBack):\n    \"\"\"\n    Get a list of the last N hours\n    \n    Returns: A list of the last N hours (in format yyyy-mm-dd hh:00)\n    \n    \"\"\"\n    hrList = []\n    for counter in range(0, numHrsBack):\n        date = (datetime.utcnow() - timedelta(hours=counter)).strftime('%Y-%m-%d %H:00')\n        hrList.append(date)\n    return hrList\n  \n    \ndef getnextNHrs(numHrsFwd):\n    \"\"\"\n    Get a list of the next N hours\n    \n    Returns: A list of the next N hours (in format yyyy-mm-dd hh:00)\n    \n    \"\"\"\n    hrList = []\n    for counter in range(0, numHrsFwd):\n        date = (datetime.utcnow() + timedelta(hours=counter)).strftime('%Y-%m-%d %H:00')\n        hrList.append(date)\n    return hrList\n\n\ndef clearBronzeDatabase():\n    listRawTables = spark.catalog.listTables(\"bronze\")\n    for table in listRawTables:\n        name = table[0]\n        fullName = \"bronze.\" + name\n        spark.sql(\"DROP TABLE {}\".format(fullName))\n    print(\"Bronze Database Cleared!\")\n    return\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9aef75fb-43af-4264-b105-c793e8bdc5ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e036eca-e776-4620-8962-81d53b25537a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilityFunctions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1059294560742899}},"nbformat":4,"nbformat_minor":0}
